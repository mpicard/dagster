---
title: Asset Materializations | Dagster
description: Dagster provides functionality to record and track changes to external assets.
---

# Asset Materializations

[Software-defined assets](/concepts/assets/software-defined-assets) provide a declarative way to define what assets should exist and how to create them. But when it's not known what asset an op is going to materialize until the op runs, you can still create assets using <PyObject object="AssetMaterialization" /> events.

"Asset" is Dagster's word for an entity, external to ops, that is mutated or created by an op. An asset might be a table in a database that an op appends to, an ML model in a model store that an op overwrites, or even a slack channel that an op writes messages to.

Op outputs often correspond to assets. For example, an op might be responsible for recreating a table, and one of its outputs might be a dataframe containing the contents of that table.

Assets can also have partitions, which refer to slices of the overall asset. The simplest example would be a table that has a partition for each day. A given op execution may simply write a single day's worth of data to that table, rather than dropping the entire table and replacing it with new data.

Dagster lets you track the interactions between ops, outputs, and assets over time and view them in the Dagit [Asset Catalog](/concepts/dagit/dagit#assets). Every asset has a "key", which serves as a unique identifier for that particular entity. The act of creating or updating the contents of an asset is called a "materialization", and Dagster tracks these materializations using <PyObject object="AssetMaterialization" /> events. These events can either be logged by the user at runtime, or automatically created by Dagster in cases where an <PyObject object="AssetKey" /> has been referenced by an op output.

## Relevant APIs

| Name                                                        | Description                                                    |
| ----------------------------------------------------------- | -------------------------------------------------------------- |
| <PyObject module="dagster" object="AssetMaterialization" /> | Dagster event indicating that an op has materialized an asset. |
| <PyObject module="dagster" object="AssetKey" />             | A unique identifier for a particular external asset            |

## Overview

There are two general patterns for dealing with assets when using Dagster:

- Put the logic to write/store assets inside the body of an op.
- Focus the op purely on business logic, and delegate the logic to write/store assets to an [IO manager](/concepts/io-management/io-managers).

Regardless of which pattern you are using, <PyObject module="dagster" object="AssetMaterialization" /> events are used to communicate to Dagster that a materialization has occurred. You can create these events either by explicitly logging them at runtime, or (using an experimental interface), have Dagster automatically generate them by defining that a given op output corresponds to a given <PyObject module="dagster" object="AssetKey" />.

## Explicit AssetMaterializations

One way of recording materialization events is to log <PyObject module="dagster" object="AssetMaterialization" /> events at runtime. These events should be co-located with your materialization logic, meaning if you store your object within your op body, then you should log from within that op, and if you store your object using an <PyObject module="dagster" object="IOManager" />, then you should log the event from your manager.

### Logging an AssetMaterialization from a Op

To make Dagster aware that we materialized an asset in our op, we can log an <PyObject module="dagster" object="AssetMaterialization" /> event using the method <PyObject object="OpExecutionContext" method="log_event" />. This would involve changing the following op:

```python file=/concepts/assets/materialization_ops.py startafter=start_materialization_ops_marker_0 endbefore=end_materialization_ops_marker_0
from dagster import op


@op
def my_simple_op():
    df = read_df()
    remote_storage_path = persist_to_storage(df)
    return remote_storage_path
```

into something like this:

```python file=/concepts/assets/materialization_ops.py startafter=start_materialization_ops_marker_1 endbefore=end_materialization_ops_marker_1
from dagster import AssetMaterialization, op


@op
def my_materialization_op(context):
    df = read_df()
    remote_storage_path = persist_to_storage(df)
    context.log_event(
        AssetMaterialization(
            asset_key="my_dataset", description="Persisted result to storage"
        )
    )
    return remote_storage_path
```

We should now see a materialization event in the event log when we execute a job with this op.

<!-- This was generated with:
    * `dagit -f materialization_ops.py -p 3333` inside docs_snippets/concepts/assets directory
-->

<Image
alt="asset-materialization"
src="/images/concepts/assets/asset-materialization.png"
width={3808}
height={2414}
/>

### Logging an AssetMaterialization from an IO Manager

To record that an <PyObject object ="IOManager"/> has mutated or created an asset, we can log an <PyObject module="dagster" object="AssetMaterialization" /> event from its `handle_output` method. We do this via the method <PyObject object="OutputContext" method="log_event" />.

```python file=/concepts/assets/materialization_io_managers.py startafter=start_marker_0 endbefore=end_marker_0
from dagster import AssetMaterialization, IOManager


class PandasCsvIOManager(IOManager):
    def load_input(self, context):
        file_path = os.path.join("my_base_dir", context.step_key, context.name)
        return read_csv(file_path)

    def handle_output(self, context, obj):
        file_path = os.path.join("my_base_dir", context.step_key, context.name)

        obj.to_csv(file_path)

        context.log_event(
            AssetMaterialization(
                asset_key=AssetKey(file_path),
                description="Persisted result to storage.",
            )
        )
```

### Attaching Metadata to an AssetMaterialization

There are a variety of types of metadata that can be associated with a materialization event, all through the <PyObject module="dagster" object="MetadataEntry" /> class. Each materialization event optionally takes a list of metadata entries that are then displayed in the event log and the [Asset Catalog](/concepts/dagit/dagit#assets).

#### Example: Op body

```python file=concepts/assets/materialization_ops.py startafter=start_materialization_ops_marker_2 endbefore=end_materialization_ops_marker_2
from dagster import AssetMaterialization, MetadataValue, op


@op
def my_metadata_materialization_op(context):
    df = read_df()
    remote_storage_path = persist_to_storage(df)
    context.log_event(
        AssetMaterialization(
            asset_key="my_dataset",
            description="Persisted result to storage",
            metadata={
                "text_metadata": "Text-based metadata for this event",
                "path": MetadataValue.path(remote_storage_path),
                "dashboard_url": MetadataValue.url(
                    "http://mycoolsite.com/url_for_my_data"
                ),
                "size (bytes)": calculate_bytes(df),
            },
        )
    )
    return remote_storage_path
```

#### Example: IO Manager

```python file=concepts/assets/materialization_io_managers.py startafter=start_marker_1 endbefore=end_marker_1
from dagster import AssetMaterialization, IOManager


class PandasCsvIOManagerWithAsset(IOManager):
    def load_input(self, context):
        file_path = os.path.join("my_base_dir", context.step_key, context.name)
        return read_csv(file_path)

    def handle_output(self, context, obj):
        file_path = os.path.join("my_base_dir", context.step_key, context.name)

        obj.to_csv(file_path)

        context.log_event(
            AssetMaterialization(
                asset_key=AssetKey(file_path),
                description="Persisted result to storage.",
                metadata={
                    "number of rows": obj.shape[0],
                    "some_column mean": obj["some_column"].mean(),
                },
            )
        )
```

Check our API docs for <PyObject module="dagster" object="MetadataEntry" /> for more details on they types of event metadata available.

### Specifying a partition for an AssetMaterialization

If you are materializing a single slice of an asset (e.g. a single day's worth of data on a larger table), rather than mutating or creating it entirely, you can indicate this to Dagster by including the `partition` argument on the object.

```python file=/concepts/assets/materialization_ops.py startafter=start_partitioned_asset_materialization endbefore=end_partitioned_asset_materialization
from dagster import AssetMaterialization, op


@op(config_schema={"date": str})
def my_partitioned_asset_op(context):
    partition_date = context.op_config["date"]
    df = read_df_for_date(partition_date)
    remote_storage_path = persist_to_storage(df)
    context.log_event(
        AssetMaterialization(asset_key="my_dataset", partition=partition_date)
    )
    return remote_storage_path
```

## Software-defined assets vs. Asset Materializations

When working with software-defined assets, the assets and their dependencies must be known at definition time. When you look at software-defined assets in Dagit, you can see exactly what assets are going to be materialized before any code runs.

Asset Materializations, on the other hand, are logged at run time. When you run an op, you find out which assets were materialized while the op is running. This allows for some flexibility, like if you wanted to determine which assets should be materialized based on the output of a previous op.
